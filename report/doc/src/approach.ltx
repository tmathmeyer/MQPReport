\section{Requirements}
\label{sec:requirements}
The set of goals discussed earlier form the basis of what the requirements for
\textit{Checksims} should be. There is concern with \textit{Checksims} that
certain types of unethical copying may go undetected by the Smith-Waterman
algorithm.  The proposed solution to this issue is to compare assignments based
on structure rather than on content. There will also be significant work
towards the goal of making \textit{Checksims} as usable as possible for course
staff, including an archiving system for comparing previous year's submissions
and a user interface to complement the command line. With these new goals in
mind, a comprehensive set of goals has been created:
\begin{itemize}
    \item \textit{Checksims} should be usable by course staff with very little
        or no training, and should produce output in a form that can be easily
        interpreted. This was a previous goal of the project that will be
        realized by creating a user interface.
    \item \textit{Checksims} should have the ability to run structure analysis
        on at least Java, C, C++, and Python. Racket is also a possibility,
        though not likely.
    \item \textit{Checksims} should be simple to run either remotely or on a
        user's own machine. Ideally, there should be no more than a few button
        presses for a member of the course staff to run
        \textit{\textit{Checksims}} against a closed submission.
\end{itemize}

\section{Approach}
\label{sec:approach}
The additions to \textit{Checksims} required some re-architecture. The previous
\textit{Checksims} algorithm abstraction only allowed for tokenization-based
algorithms. Since the structure comparison algorithms are based on trees rather
than lists, the \textit{Checksims} architecture had to be redesigned.  In this
redesign, an abstraction that we call a ``Percentable'' was used. This name
comes directly from the code, and means any structure from which a percentage
can be calculated.  Submission text can be transformed into different types of
percentables and memoized\footnote{Memoization is a technique in programming
where results of previous computations are cached so that they must not be
calculated multiple times} by their type. Algorithms are now based on the
percentable type on which they operate and are responsible for informing the
\textit{Checksims} runner which type of percentable generator they would like
to use. The new architecture should allow a significantly broader range of
algorithms to be implemented against the underlying comparison mechanism. A
description of the updated \textit{Checksims} pipeline is as follows:

\begin{enumerate}
 \item Student submissions are read from the file system, and multi-file
     submissions are concatenated into a single string.
 \item Submissions are passed into a \textit{common code remover}, which
     removes code designated as ``common'' from all submissions to ensure that
     it is not factored into the comparison process. The \textit{common code
     remover} is based on the Smith-Waterman algorithm, which performs superbly
     for exact matching scenarios.
 \item The percentable generator provided by the selected algorithm consumes a
     submission and stores the parsed or tokenized form back into the
     submission for the purposes of memoization. This occurs on all
     submissions, but due to memoization, it will not happen more than once per
     submission.
 \item Submissions are grouped into pairs. The selected algorithm, or
     \textit{similarity detector}, which implements a \textit{similarity
     detection algorithm}, is then run on all possible pairs of submissions,
     and the results are recorded in a \textit{similarity matrix}. This also
     includes comparing the current submissions to archived submissions, though
     not comparing the archived submissions amongst each other.
 \item The similarity matrix is passed to a user-selected \textit{output
     strategy}, which produces a human-readable form of the output.
\end{enumerate}

\subsection{Parsing}
\label{sec:parsing}
Parsing is the process of taking raw data, usually in the form of plain text,
and converting it into a structure that can be operated on more easily. This is
key to performing AST analysis because it is the process by which student
submissions are transformed into ASTs. Parsing is a very difficult task
however, and writing an objectively and provably correct parser from scratch
was not a feasible task for this project. Considering this restriction, the
decision was made to use \textit{ANTLR}, a parser-generator created by
Professor Terrence Parr of the University of San Francisco~\cite{antlr-lang}.
Parser generators work by allowing a grammar to be specified in the form of a
file that specifies the constructs that are valid in the target language. ANTLR
then takes a grammar and converts it into a parser written in either C\# or
Java.  As \textit{Checksims} is written in Java, it was the obvious and logical
choice. While not as difficult to write as a parser itself, creating a correct
grammar for a language is a difficult task. A grammar file for Java8 alone is
upwards of 1500 lines long and is highly complex.

So that AST analysis may be usable on the original target languages of Java, C,
C++, and Python, grammars for each of these languages must be created. To
complicate matters further, language specifications are fluid; in 2014 the C
and Java language specifications were updated, and Java is set to have another
in September 2016.  Python is in somewhat of a civil war between versions 2.7
and 3.4 (soon to be 3.5!), and the decision was made to target the newest
version, as it is the version taught at WPI\@. Changes to a language
specification require significant overhauling of a grammar, parser, and
parser-AST interoperability layer. With this concern in mind, the grammar and
parser-AST interoperability layers are being designed to be as modular as
possible. It is unavoidable therefore that there will need to be future work
done on \textit{Checksims} to keep the parsers and interoperability layers up
to date with the language specifications.

\subsection{AST Interoperability Layer}
\label{sec:interopLayer} The ANTLR parser generator creates structures called
\textit{Parser Rule Contexts} \textit{(PRCs)}. While these structures exist in
a tree based form, they are hard to work with and contain extra meta data that
must be removed in order to quickly and accurately perform the comparison
techniques. To solve this problem, \textit{Checksims} uses a consistent and
language independent AST representation with support for tagging and
fingerprinting. To convert between a \textit{PRC} and a \textit{Checksims} AST,
every parser must also have an associated \textit{Treewalker} --- an object
designed to examine every node of a \textit{PRC} tree and convert it to a
\textit{Checksims} AST\@.

\subsection{Comparison Techniques}
\label{sec:comparisonTechniques}
Once student submissions have been converted from source files into ASTs, they
must be compared. Several algorithms have been developed for comparing trees,
the results of which will be discussed below. Each algorithm will convert a
pair of ASTs into an instance of the previously mentioned \textit{Percentable},
which the \textit{Checksims} core will handle as it currently does.

\subsubsection{AST Fingerprinting}
ASTs are by definition abstract --- in their raw form they have very little
contextual information, and as such many re-orderings and different
combinations may have the same or similar meanings. A comprehensive way to
compare nodes in an AST is through a method called fingerprinting. This method
involves using one AST to build a ``dictionary'', or mapping, from every node
to a fingerprint of that node. Using the generated dictionary, it is possible
to traverse any other AST, and search all nodes for similar counterparts
efficiently. This method eliminates the need to compare conceptually different
nodes, such as classes and control blocks. This method also ignores nodes that
are otherwise dissimilar, such as functions with different argument counts. The
fingerprint of each node may be calculated using heuristics and based on such
features as child count, specialized type tags on tree nodes, and even the hash
values of children. The selected technique involved XORing a cryptographic hash
of the type of tree node together with the fingerprint of every child AST\@.
This technique is called a ``rolling checksum'' as each fingerprint / checksum
is calculated by recursively operating down the tree. The exact code used for
generating fingerprints can be found in Appendix A. The following code is
transformed into an AST, on which hash values are calculated in a rolling
fashion. Both the code and the AST with hash values are shown below.

\begin{figure}
    \begin{lstlisting}
x = 2
y = 3
if x<y:
  print ``y is bigger than x''
    \end{lstlisting}\label{fig:ast}
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{./img/ast.png}
    \caption{In this example, it is shown how a basic sample of code is
    transformed, as well as showing the parent connections of nodes and their
    names and fingerprints.}
\end{figure}

\begin{figure}\label{fig:ast_same}
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{./img/ast_same.png}
    \caption{In this example, it is shown how the rolling checksum affects
        nodes with the same name. Note that the two lowest nodes, each named
        ``SAME'' have the same fingerprint. The second node on the second row
        and the first node on the first row also have the name ``SAME'',
        however each of these have radically different fingerprints than the
        two lowest nodes with whom they share a name. Fingerprints in both of
        these examples are generated using code identical to that which is
        contained in \textit{Checksims}.}
\end{figure}
\clearpage

\paragraph{Fingerprint Collision}
During the implementation and testing of the fingerprinting method, it was
discovered that if two nodes in the AST had the same fingerprint (referred to
as a hash collision or fingerprint collision), one of the mappings would be
lost when stored in the fingerprint dictionary. An attempt at fixing this
problem was made, however this caused the execution time to increase by more than one
hundred fold, as truly similar nodes were being kept separately.

After more extensive testing, it was discovered that almost all instances of
hash collisions occurred on identical nodes in the AST\@. This was considerable
evidence that there is no need to worry about collisions. Mathematically, for
any given tree with 10000 nodes, the chance of collision is 1.16 percent
\footnote{This is a variation on the Birthday Problem. The
    probability of collision for $n$ nodes can be found by
    $p(n)=1-\frac{I!}{I^n*(I-n)!}$ where $I$ is the number of values that a
hash could be, in this case, $I = 2^{32} -1$}.\ For a tree with 20000 nodes,
the chance is 4.55 percent. The average number of nodes per submission in the
the first corpus of data was 7952. The chance of a collision in samples of this
size is 0.669 percent. These calculations provide final evidence that there
there is no need to account for collisions.

\subsubsection{Recursive Structure Percentage Comparison}
Recursively comparing nodes of the AST is another effective algorithm for
similarity detection. In this method, each node in the AST is marked as
``ordered'' or ``unordered''. The similarity of two ordered nodes is a direct
average of their children. The similarity of two unordered nodes is computed by
a function of the intersection of their child nodes. The similarity of an
ordered and unordered node would be calculated through applying scoring
heuristics to a sorted intersection of the child nodes. Recursive comparison
techniques are significantly slower than fingerprinting methods and generally
provided no additional benefits, making them far less practical for use on real
world data.

\section{User Interface}
\textit{Checksims} version 1.2 operates exclusively as a command line utility;
it is usable only by those who learn the necessary flags and their meanings.
While this does not impose any technical limitations, it makes
\textit{Checksims} more difficult to use for end users. A GUI significantly
simplifies the process of using \textit{Checksims} routinely for assignments.

\subsection{Mapping the CLI to a GUI}
\textit{Checksims} has both optional and required flags, some of which take
multiple arguments. To transform this into a graphical user interface, the
required flags will be central to the view of the user. Optional flags may be
hidden under an ``Advanced'' or other menu. In its current state,
\textit{Checksims} requires only three flags:
\begin{itemize}
    \item -s, --submissiondir [source,[sources]]: a list of directories
    \item -a, --algorithm [name]: a \textit{Checksims} algorithm
    \item -o, --output [name]: an output type
\end{itemize}
Since the GUI will be capturing the output, only the submissions and algorithm
will have to be configurable through the interface. Submission directories must
be valid directories or zip files on a user's computer; users will be prompted
in the form of a file selection dialog.  Algorithms may only be selected from a
set of valid algorithms implemented by \textit{Checksims}, therefore, users
will be prompted in the form of a populated drop-down menu. A mock up of the
\textit{Checksims} launcher GUI is shown below.
\begin{figure}\label{fig:gui_mockup}
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{./img/mockup.png}
    \caption{This mock up uses the WPI Colors, as set forth in the WPI Logo
    Usage Guide~\cite{wpi_logo_guide}. It is intended that the comparison
    process will be initiated when the user presses the ``Run Checksims''
    button.}
\end{figure}
\newpage

\subsection{Displaying Output in a GUI}
After \textit{Checksims} has finished calculations, it must render the results
in a human readable form. Traditionally, this has been done through either the
threshold or HTML displays. The \textit{Checksims} GUI should have an option to
switch between these two types of displays, while providing a more rich user
experience beyond what the command line output interfaces provide.
