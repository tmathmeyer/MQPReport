\section{Results}
\subsection{AST Compare vs Smith Waterman}
\subsubsection{Runtime}
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{Algorithm Runtimes on S1, Milliseconds}\label{runtimes1} 
    \begin{tabular}{@{}lll@{}}
        \toprule
        Name    & Smith-Waternan & AST Comparison \\ \midrule
        3.a13.1 & 2640           & 2566 \\
        3.a13.2 & 8657           & 3103 \\
        3.a13.3 & 2640           & 2566 \\
        3.a12.1 & 2406           & 1718 \\
        3.a12.2 & 11978          & 3951 \\
        3.a12.3 & 6070           & 3640 \\
        4.a15.5 & 1034           & 2912 \\
        4.a15.6 & 39587          & 4300 \\
        4.c15.4 & 2315           & 3371 \\
        4.c15.5 & 1211           & 2360 \\
        4.c15.6 & 35008          & 7324 \\ \bottomrule
    \end{tabular}
\end{minipage}
\\
\\
\input{runtime_plot.ltx}
\\
\\
These results show the number of miliseconds that it took for
\textit{Checksims} took to run on many corpora on machine S1. The results show
that while the AST Comparison algorithm is slower than Smith Waterman for small
assignments, on larger assignments, AST comparison can be five times faster
than Smith Waterman. It is expected that this difference in runtime will be
even greater in favour of AST Comparison on even larger assignments.
\subsubsection{Identified Instances of Similarity}
TODO

\subsection{AST Compare vs MOSS}
\subsubsection{Runtime}
MOSS takes on average 14.5 seconds longer to run than the AST Comparison
algorithm on machine S1. Closer inspection of runtime shows that this is almost
entirely due to the process of uploading sometimes hundreds of files to the
Stanford University server where MOSS is run. Subtracting the runtime of the
upload step, the average runtime actually changes to 0.8 seconds in favor of
MOSS\@.  MAKE A GRAPH AND PUT IT HERE!!!

\subsubsection{Identified Instances of Similarity}
for corpora 3.a13.1, 3.a13.2, 3.a13.3, 3.a12.1, 3.a12.2, and 3.a12.3, MOSS and
AST Compare reported the same set of pairs as being likely matches. When corpus
2.b14.2 was provided to the AST Compare algorithm, 4 pairs of assignments were
marked as being highly likely to contain instances of copying; that is, both
the matching value and inverse matching value between both assignments were
greater than 70 percent. MOSS only managed to catch a single pair of
assignments however. The significant pairs, in which either assignment matches
another greater than 69 percent are shown:
\\
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{MOSS vs AST Compare, Corpus 2.b14.2}\label{mossvast} 
    \begin{tabular}{@{}lll@{}}
        \toprule
        Assignments          & AST Compare percentages & MOSS percentages  \\ \midrule
        students 229 and 21  & 81 and 83 percent       & 60 and 60 percent \\
        students 179 and 176 & 74 and 79 percent       & 17 and 17 percent \\
        students 196 and 219 & 73 and 71 percent       & 23 and 17 percent \\
        students 211 and 191 & 72 and 69 percent       & nothing reported  \\
                             &                &      \\ \bottomrule
    \end{tabular}
\end{minipage}
\\
\\
While Checksims with AST Comparison reports a significantly higher average
percentage maxed for assignments, the values reported by MOSS are reletively
low. All four assignments have been manually reviewed as well and it was
determined that theese students would have been called in to discuss academic
dishonesty with the course staff. That MOSS missed these assignments provides
significant evidence to show that AST Comparison is at least as good, if not
better, than the closed source algorithm that powers MOSS\@.


















\section{Old Results --- here for temp reference}
\subsection{Time Complexity}
\subsubsection{Smith-Waterman}
When running \textit{Checksims} with the Smith-Waterman algorithm on corpus
2.b14.2 and machine S1 the process takes 7 minutes and 36.55 seconds.

\subsubsection{AST Fingerprinting}
Running the AST Fingerprinting algorithm on corpus 2.b14.2 and machine S1 takes
41.78 seconds.

\subsection{Flagged Submissions}
\subsubsection{Smith-Waterman}
On corpus 2.b14.2, Smith-Waterman detected 31 instances of plagerism. Upon
manual inspection, many of these instances involved student 33, who only
submitted a single file with only 78 lines of code, many of which were short
and contained the specific keywords students were instructed to use. The
inverse matching value for 26 of these 31 instances were less than 20 percent,
and can be considered false positives. After these were removed, the pairs
still flagged for similarity are:
\begin{itemize}
    \item students 9 and 45
    \item students 229 and 21
    \item students 195 and 80
    \item students 56 and 148
    \item students 176 and 179
\end{itemize}

\subsubsection{AST Fingerprinting}
On corpus 2.b14.2, AST Fingerprinting detected 15 instances of plagerism. On
closer inspection, 4 pairs were found to have a match value and inverse match
value greater than 70 percent:
\begin{itemize}
    \item students 219 and 196
    \item students 191 and 211
    \item students 21 and 229
    \item students 176 and 179
\end{itemize}
As mentioned earlier, AST parsing is impossible for students who submit invalid
code. When running the AST Fingerprinting algorithm on corpus 2.b14.2, the
following submissions were found to have errors:
\begin{itemize}
    \item student 186
    \item student 71
    \item student 65
    \item student 33
    \item student 133
    \item student 9
    \item student 184
    \item student 240
    \item student 59
\end{itemize}

\subsubsection{Comparing the methods}
The results of the algorithms show that each caught in some way a similar set
of assignments. AST Fingerprinting missed catching students 195 and 80, and 56
and 158. Smith Waterman missed 219 and 196, and 191 and 211. Upon manual
inspection of these assignments, the following conclusions were made:
\begin{itemize}
    \item students 195 and 80 copied code
    \item students 56 and 158 did not copy code
    \item students 219 and 196 copied code
    \item student 191 and 211 copied code
\end{itemize}
These results were signiifcant because they show that neither algorithm catches
all instances of plagerism, and that the AST Comparison method is credible by
itself, but also in tandem with Smith Waterman.

