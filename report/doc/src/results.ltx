\section{Results}
\subsection{AST Compare vs Smith Waterman}
\subsubsection{Execution time}
The time taken by \textit{Checksims} to identify similarities is heavily
dependent upon the number of assignments being tested and by the algorithm used
to do the comparison. The following graphs show that the AST comparison
algorithm is significantly faster than Smith Waterman. While the amount of time
taken is rather trivial for smaller assignments, it is plausible to assume that
\textit{Checksims} may be run on a corpus with many hundreds of assignments; at
such a scale, the execution time may be many orders of magnitude different.
\input{runtime_plot_small.ltx}
\\
\\
This graph shows the number of seconds taken by both Smith Waterman and AST
Comparison to compute a similarity matrix for corpora with fewer than 100
assignments. A best fit line was constructed for each set of points, showing
that AST Fingerprinting ranges anywhere from 33 percent to 75 faster than the
Smith Waterman Algorithm. The raw data for this graph can be found in Appendix
E.
\\
\\
\input{runtime_plot_large.ltx}
\\
\\
This graph shows the number of seconds taken by both Smith Waterman and AST
Comparison to compute a similarity matrix for corpora with more than 100
assignments. A best fit line was constructed for each set of points, showing
that AST Fingerprinting ranges from 75 percent to 95 percent faster than
Smith Waterman. Raw data for this graph can be found in Appendix E.
\clearpage
\subsubsection{Identified Instances of Similarity}
\paragraph{In Python 3}
The five Python 3 corpora were each tested with both Smith Waterman and AST
Comparison. In corpus 4.a15.6 and 4.c15.6, neither Smith Waterman nor AST
Comparison identified any instances of similarity.

In corpus 4.a15.5, AST Comparison identified similarities between students 14
and 64, which were verified by hand; Smith Waterman did not detect any
similarity in this corpus.

In corpus 4.c15.5, AST Comparison identified similarities between
\begin{itemize}
    \item students 12 and 16
    \item students 18 and 34
    \item students 42 and 69
\end{itemize}
Smith Waterman identified similarities between
\begin{itemize}
    \item students 12 and 16
    \item students 10 and 53
\end{itemize}
Manual inspection of these four pairs of assignments show that it is highly
likely that the students in question were involved in unauthorized copying.

In corpus 4.c15.4, Smith Waterman
detected similarity between four pairs of students: 
\begin{itemize}
    \item students 54 and 8
    \item students 54 and 4
    \item students 71 and 20
    \item students 41 and 18
\end{itemize}
On this same corpus, AST Comparison detected similarities between the
following pairs of students:
\begin{itemize}
    \item students 54 and 8
    \item students 54 and 4
    \item students 41 and 18
    \item students 4 and 8
    \item students 84 and 42
    \item students 0 and 17
    \item students 83 and 79
\end{itemize}
Students 71 and 20 did not compile due to syntax errors created during the
anonymization process, as student names were included in parts of the code. AST
Analysis was therefore not able to have been run on this pair. It is
interesting that while Smith Waterman detected similarities between the pair
(54 and 8) and the pair (54 and 4), it did not detect similarity between the
pair (4 and 8), while AST Comparison did. Manual verification of these
assignments shows that all of them were likely instances of unauthorized
copying, though this was disputed for the pair of students 84 and 42.
\paragraph{In Java}
The single Java corpus (!!TODO!!) was tested with both Smith Waterman and AST
Comparison. Smith Waterman detected similarities between the following:
\begin{itemize}
    \item students 229 and 21
    \item students 9 and 45
    \item students 195 and 80
\end{itemize}
AST Comparison detected similarities between:
\begin{itemize}
    \item students 229 and 21
    \item students 179 and 176
    \item students 196 and 219
    \item students 191 and 244
\end{itemize}
Manual inspection of these six pairs of assignments shows that student 9 did
not submit valid code, and could not be processed by AST Comparison, but was
most likely involved in a case of unauthorized copying with student 45.
Students 195 and 80 appeared to be very similar, but there were enough
differences to conclude that these two students did not engage in unauthorized
copying. All other four pairs were inspected and were found to have almost
certainly engaged in unauthorized copying.
\paragraph{In C89}
\paragraph{In C++11}

\subsection{AST Comparison vs MOSS}
\subsubsection{Execution Time}
MOSS takes on average 14.5 seconds longer to run than the AST Comparison
algorithm on machine S1. Closer inspection of execution time shows that this is
almost entirely due to the process of uploading sometimes hundreds of files to
the Stanford University server where MOSS is run. Subtracting the execution
time of the upload step, the average execution time actually changes to 0.8
seconds in favor of MOSS\@.  MAKE A GRAPH AND PUT IT HERE!!!

\subsubsection{Identified Instances of Similarity}
For corpora 3.a13.1, 3.a13.2, 3.a13.3, 3.a12.1, 3.a12.2, and 3.a12.3, MOSS and
AST Compare reported the same set of pairs as being likely matches. When corpus
2.b14.2 was provided to the AST Compare algorithm, 4 pairs of assignments were
marked as being highly likely to contain instances of copying; that is, both
the matching value and inverse matching value between both assignments were
greater than 70 percent. MOSS only managed to catch a single pair of
assignments however. The significant pairs, in which either assignment matches
another greater than 69 percent are shown:
\\
\\
\begin{minipage}{\linewidth}
    \centering
    \captionof{table}{MOSS vs AST Compare, Corpus 2.b14.2}\label{mossvast} 
    \begin{tabular}{@{}lll@{}}
        \toprule
        Assignments          & AST Compare percentages & MOSS percentages  \\ \midrule
        students 229 and 21  & 81 and 83 percent       & 60 and 60 percent \\
        students 179 and 176 & 74 and 79 percent       & 17 and 17 percent \\
        students 196 and 219 & 73 and 71 percent       & 23 and 17 percent \\
        students 211 and 191 & 72 and 69 percent       & nothing reported  \\
                             &                &      \\ \bottomrule
    \end{tabular}
\end{minipage}
\\
\\
All four assignments have been manually reviewed as well, and it was determined
that these students would have been called in to discuss academic dishonesty
with the Professor. That MOSS missed these assignments provides significant
evidence to show that AST Comparison is at least as good, if not better, than
the closed source algorithm that powers MOSS\@.
