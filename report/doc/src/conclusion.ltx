\section{Conclusion}
Smith-Waterman, AST Comparison, and MOSS each flagged assignments which the
others did not. In addition to this, each flagged a hanfull of false positives,
showing that these tools are not a substitute for manual inspection.
\subsection{Strengths of AST Fingerprinting}

\subsubsection{Runtime}

AST Fingerprinting is significantly faster than Smith-Waterman in almost every
situation, and the significance of this difference is greater in evaluations
with many individual submissions.  AST Fingerprinting is also faster than MOSS
for large assignments; however for evaluations with \textasciitilde20 or fewer
individual submissions of small size, MOSS is slightly faster --- it is usually
about 1 to 10 seconds faster.

\subsubsection{Flagged Submissions}

AST Fingerprinting has shown itself to be more effective than Smith-Waterman at
finding pairs of assignments which are examples of unauthorized copying. It is
also more effective in terms of flagging fewer false positives in this regard.
AST Fingerprinting is able to compete with MOSS in terms of results, though as
discussed below there are issues with invalid sources.

\subsection{Shortcomings of AST Fingerprinting}

While AST Fingerprinting has proven to be a significant improvement over
Smith-Waterman in many areas, and can even compete fairly well with MOSS, there
are issues that it faces.

\subsubsection{Memory Usage}

Often in algorithms there is a trade off between running quickly and using
large amounts of memory. The nature of AST Fingerprinting is that it uses a
large amount of memory, which can have severe side effects on older computers.
Many older computers, especially laptops, do not have enough memory to run the
\textit{Checksims} algorithms on a large corpora. In this situation the
operating system performs an action called ``swapping'', where it uses the hard
drive as a buffer for RAM\@. This can severely decrease the performance of
\textit{Checksims}.

\subsubsection{Invalid Assignments}

Because AST Fingerprinting uses a very basic compiler, it is necessary that all
submissions actually be valid syntax for whichever language they are targeting.
Examination of the corpora has shown that two to four percent of students
submit code that is invalid. Student submissions that were flagged by
\textit{Checksims} were checked manually with a standard compiler for
correctness.  In a class of 300 students, this leaves anywhere from six to
twelve students that cannot be tested with this algorithm.

\subsubsection{Individual Assignments Inspector}

One of the places here both AST Fingerprinting and Smith-Waterman fall short of
MOSS is inspecting two assignments side by side.  This is a significant
challenge, and there was not enough time in this project to implement it.

\subsection{The Graphical User Interface}

The second stage of this project was to implement a GUI for displaying the
results generated by \textit{Checksims}. This GUI supports features such as
filtering, sorting, exporting, and inspecting individual results. The GUI has
been created using the Java Swing framework, and is cross platform like the
original \textit{Checksims} command line interface.
