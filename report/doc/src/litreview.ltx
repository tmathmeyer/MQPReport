\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{framextopmargin=5pt,frame=tb}

\section{Existing Literature}
A large portion of the work done is based on the work put forth in the
original \textit{Checksims} paper. This includes not only the code that was
produced in that project, but also the definitions and literature review that
was conducted. Many of the definitions and terms discussed from here on can be
directly attributed to their work.

\subsection{Academic Dishonesty}
The original \textit{Checksims} authors provided a comprehensive set of
examples and scenarios where academic dishonesty is the hardest to
distinguish. As they noted, an instance of ``100\% similarity was almost
certainly indicitive of unauthorized copying''\cite{checksims1}, but there are
many scenarios where determining whether a submission constitutes dishonest
behavior is not as simple.
% Throw in a few papers here, discuss various standards.

\subsection{Detecting Academic Dishonesty}
There have been numerous approaches to detecting duplicate code. Primarily,
researchers attempt to identify academic dishonesty, duplicated code in
codebases, or copyright infringement in various contexts. While different
goals, these methods share the same task: to identify duplicated despite
obfuscation tactics. This section discusses the types of identification 
methods and particular methods developed while reviewing existing literature.

\subsubsection{Detection of Dishonesty using Body Text}
The original \textit{Checksims} paper thoroughly reviews a number of existing
methods of identifying duplicate code. As \textit{Checksims} was dedicated
to language independent identification, it did not discuss syntax tree based
methods.

\subsubsection{Detection of Dishonesty using Syntax Trees}
As was again mentioned in the original \textit{Checksims} paper, detecting
academic dishonesty in programming assignments is significantly easier than
detecting dishonesty in natural language due to the fact that programming
languages follow grammars which are significantly simpler than natural written
language. Consider the grammar for the LISP family of programming languages;
in the grammar definition syntax of ANTLR, a lisp program would be defined as
the following:

\begin{lstlisting}
COMMENT : \;.*\n -> skip
ID : [a-zA-Z0-9]+

sExpr :
    '(' (ID | sExpr)+ ')'
\end{lstlisting}

Written long hand, this grammar states that any block of text between a
semicolon (;) character and a newline (\textbackslash{} n) character is
considered a comment and can be skipped for the purpose of parsing. An ``ID''
consists of any arrangement of the characters a---z, A---Z, and 0--9, and than
an ``sExpr'' consists of at least 1 child, where each child is either an ID or
an sExpr.

This grammar definition shows that LISP languages are just textual
representations of an N-dimensional tree, with IDs as leaf nodes and sExprs as
branch nodes. The difficulty of detecting dishonesty by transforming text into
an AST is that if the submitted text does not correctly implement the expected
grammar, a tree cannot be generated and comparison cannot happen. This is
surprisingly common in student submissions for a class --- real world examples
used for testing show that about 2 to 4 percent of code submissions are not
valid for the language in which they are written.

% Throw in methods here from various papers.
\paragraph{Using the Vector Space Model on Syntax Trees}
The STVsm paper posed an interesting syntax tree based approach to similarity
detection. In the STVsm paper, an AST was generated from C code. This AST was
then transformed back into a sring of s-expressions. To compare any two
documents, multiple string similarity techniques could be applied, including
Levenshtein Distance, Greedy String Tiling, and Vector Space Model method.
This idea is novel for a number of reasons. It is based on the syntax tree of
the associated language, but it does not directly rely on comparing the
structure of the syntax trees. Instead, it generates a ``sanitized''  version
of the document, and uses generic text comparison methods to detect
similarities. This allows the methods to ignore plaintext obfuscation that
would not appear on the AST, and had improved results over plaintext
comparison methods.
%STVsm: Similar Structural Code Detection Based on AST and VSM
%http://download.springer.com/static/pdf/963/chp%253A10.1007%252F978-3-642-35267-6_3.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-642-35267-6_3&token2=exp=1444506960~acl=%2Fstatic%2Fpdf%2F963%2Fchp%25253A10.1007%25252F978-3-642-35267-6_3.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Fchapter%252F10.1007%252F978-3-642-35267-6_3*~hmac=121b59496e4d1e62e2cc3e7031f4bd7bb7000b391b69e0d88e1efdbd7d58e0f9

\paragraph{Syntax Tree Fingerprinting}
Syntax Tree Fingerprinting presents an efficient method of locating
structurally similar portions of two syntax trees. The fingerprinting
technique stores each tree in two forms: an original AST structure, but
also as a mapping from ``fingerprint'' to a node of the tree. This mapping
must only be generated once, as it is not modified during the comparison
process. Two tree A and tree B, every node in tree A must be hashed, and then
using this hash a lookup must be done on the mapping for tree B. If the node
from A is the same as the result of the lookup in B, then a weighted score
can be calculated. This opens the door for many techniques in fingerprinting,
including cryptographic hashes and rolling checksums, as well as techniques in
weighting the scores for each node.
%Syntax tree fingerprinting: a foundation for source code similarity detection

%http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=99955C2F6108740C9101091A51AB51A7?doi=10.1.1.393.2183&rep=rep1&type=pdf
