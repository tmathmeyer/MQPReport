\section{Comparison to Smith-Watermann}
\label{sec:discussComparison}
By creating an effective AST comparison tool, it is hoped to be able to fill
theoretical gaps that may exist in the Smith-Watermann algorithm for code
comparison. We would like to see what the strengths and weaknesses of each
algorithm are, especially in the realms of false negatives and false positives.
There are other factors that affect the feasability of different algorithms,
including execution speed and memory performance. Preliminary tests show that
AST analysis is significantly slower than simple line, space, or character
tokenization, but is also more memory efficient and would scale better over
sample sizes in the hundreds. Results of the comparisons between the two
algorithms will be discussed further on in the report.

\subsection{Time Complexity}
\label{sec:comparison-timecomplex}
\subsubsection{Big-O Notation}
The Checksims pipeline works by first reading in and concatinating every file
in each student's assignment. The next step, and first in the comparison
process, is to parse that text. This is a linear process; the amount of time
taken to parse every assignment corresponds linearly to the number of
assignments. The second step is to compare each current assignment to every
other current assignment, and each current assignment to each historical
assignment. This process corresponds quadradically to the number of assignments
present and the size of those assignments. In Mathematics and Computer Science,
these relationships are referred to as $O(n)$ and $O(n^{2})$ respectively. 

\subsubsection{Smith-Waterman}
The Smith-Waterman algorithm consists of two parts, each run separately by
\textit{Checksims}. The first step is to split the submission text by any of
three delimiters: newlines, space characters (such as space and tab), or at
every character. As discussed previously, this step runs in $O(n)$ time, so it
does not have a great effect on the overall runtime. The second step is to run
the lines through a comparison matrix and compute similar sequences. This is a
time consuming process, and because this step runs in $O(n^{2})$ time, it has a
significant impact on the overall runtime of the \textit{Checksims} program.

\subsubsection{AST Fingerprinting}
The simple implementation of AST fingerprinting also consists of two parts,
much like Smith-Waterman. The first step however is incredibly time consuming,
as a tree must be constructed from the submitted source code. As discussed
previously, this is very time consuming process. This result is also cached
however, so each assignment needs only to be parsed once, and because it is
part of the $O(n)$ step of the process, it has a relatively small impact on the
overall performance. Where AST Fingerprinting excels is in the comparison,
which is simply a sequence of hashmap lookups and some arithmetic. As this step
is run $O(n^{2})$ times, its runtime is significant to the overall runtime.
Because the AST fingerprinting process does not do expensive calculations
during the $O(n^{2})$ part of the process, we can expect it to scale better
than the Smith-Waterman algorithm; that is, for large assignments, we expect
AST fingerprinting to be significantly faster than Smith-Waterman.

\subsection{Relationship to Manual inspection}
Based on previous experience as course staff and having manually inspected
assignemnts for many classes, The two main things that signal unauthorized
copying are structual similarities and obvious and repeated typographical
errors. Structual errors include such visual details such as indentation and
spacing between lines of code, as well as the organisation of control-flow
withing the program. AST Fingerprinting is designed to detect similarities in
control-flow organisation, but may not pick up typographical errors, as it has
no way of comparing the spelling of keywords or symbols. Smith-Waterman does
exactly the opposite of this, detecting only similiarities in spelling and
using tokenisation to completely ignore structural differences. Prior
experience as teaching staff have shown that manually comparing structure
yields a more accurate indication of unauthorized copying when compared to
typographical comparisons.

\section{Comparison to MOSS}
The Measure of Software Similarity, often referred to as MOSS, is the gold
standard for similarity detection. MOSS is developed by Professor Alex Aiken
from Stanford University. It is provided as a webservice, and is a closed
source tool. Due to the closed nature of MOSS, it is hard to effectively
compare the AST comparison algorithm to it; it is only possible to compare
results and runtime. The standard way of interfacing with MOSS is through a
perl script, provided to a user apon registering for the service. We feel that
the checksims model and command line tools are superior in the field of
usability. MOSS often had problems with network timeouts and simply failing to
report any results at all.  As checksims is run locally, it is not subject
failed or slow network connections.  Perhaps most importantly, The ability to
run checksims locally removes any ethical question of uploading named student
assignments to a remote machine that is not under the control of course staff.
