\section{Comparison to Smith-Watermann}
\label{sec:discussComparison}
The need to create an AST comparison tool was realized when it was shown that
certain obfuscation techniques could be applied to blatantly plagerized code in
order to defeat the Smith-Waterman algorithm. An example scenario was created
in which a medium sized Java file was created by copying a source file and
renaming every variable and class type. It was shown that this type of simple
obfuscation was enough to cause \textit{Checksims} to fail to recognize
plagerism that would be fairly obvious to a human. By creating an effective AST
comparison tool, it is hoped that theoretical gaps in the Smith-Watermann
algorithm are brought to light. The goal of comparing Smith-Waterman to an AST
comparison algorithm is to show the strengths and weaknesses of each when
applied to real world examples. This includes metrics like runtime and memory
consumption, in addition to false positive and missed instances of plagerism.
The results measured from both of these algorithms are discussed further on in
this paper.

\subsection{Time Complexity}
\label{sec:comparison-timecomplex}
\subsubsection{Execution runtime}
The Checksims pipeline works by first reading in and concatinating every file
in each student's assignment. The next step, and first in the comparison
process, is to parse that text. This is a linear process\footnote{Linear
runtimes are expressed as $O(n)$ in Big-O\cite{big-o} notation.}; the amount of
time taken to parse every assignment corresponds linearly to the number of
assignments. The second step is to compare each current assignment to every
other current assignment, and each current assignment to each historical
assignment. This process corresponds quadradically\footnote{Quadratic runtimes
are expressed as $O(n^2)$ in Big-O notation.} to the number of assignments
present and the size of those assignments.

\subsubsection{Smith-Waterman}
The Smith-Waterman algorithm consists of two parts, each run separately by
\textit{Checksims}. The first step is to split the submission text by any of
three delimiters: newlines, space characters (such as space and tab), or at
every character. As discussed previously, this step runs in $O(n)$ time, so it
does not have a great effect on the overall runtime. The second step is to run
the lines through a comparison matrix and compute similar sequences. This is a
time consuming process, and because this step runs in $O(n^{2})$ time, it has a
significant impact on the overall runtime of the \textit{Checksims} program.

\subsubsection{AST Fingerprinting}
The implementation of AST fingerprinting also consists of two parts, much like
Smith-Waterman. The first step however is reletively time consuming, as a tree
must be constructed from the submitted source code. This result is cached so
that each assignment needs only to be parsed once. This overall has a
reletively small impact on the overall performance because it is part of the
$O(n)$ step of the process. Where AST Fingerprinting excels is in the
comparison, which is simply a sequence of hashmap lookups and some arithmetic.
As this step is run $O(n^{2})$ times, its runtime is significant to the overall
runtime. Because the AST fingerprinting algorithm does not do expensive
calculations during the $O(n^{2})$ step, we can expect it to scale better than
the Smith-Waterman algorithm; that is, for large assignments, we expect AST
fingerprinting to be significantly faster than Smith-Waterman.

\subsection{Relationship to Manual inspection}
Based on the previous experience of one of the authors (TJM) as a member of
course staff, and his having manually inspected assignments for many classes,
two main things that signal unauthorized copying can be identified. Structual
similarities are one such signal, and include such visual details such as
indentation and spacing between lines of code, as well as the organisation of
control-flow within the program. AST Fingerprinting is designed to detect
similarities in control-flow organisation, but may not pick up the other main
signal of unauthorized copying: typographical similarities such as the spelling
of words and identifier names. Smith-Waterman does exactly the opposite; it
detects only similiarities in spelling and using tokenisation to completely
ignore structural differences. Prior experience as teaching staff have shown
that manually comparing structure yields a more accurate indication of
unauthorized copying when compared to typographical comparisons.

\clearpage
\subsection{Example of Plagerism}
\input{codecompare.ltx}
This code is a subset of the code submitted by students 14 and 64 for
assignment five in the Introduction to Computer Science for Non--Majors, taught
in A term 2015 using python. While these snippets of code are not identical,
upon examination it becomes apparent that the only difference between them is
the names of functions and variables. The other four files in this submission
are similar in this regard as well.  When the Smith-Waterman algorithm is used
to compare these two assignments, they have a reported similarity percentage of
32 and 34 percent. When the AST Comparison algorithm is used, the reported
similarity is 100 percent in both directions. If this pair of students had been
caught while the course was in session, they most certainly would have been
asked to talk to the professor about the blatant attempt at not only copying
code in an unauthorized manner, but attempting to hide the fact that they did
so. This ramifications of this results are discussed in the results section of
the paper.

\section{Comparison to MOSS}
The \textit{Measure of Software Similarity}\cite{MOSS}, often referred to as
MOSS, is the gold standard for similarity detection. MOSS is developed by
Professor Alex Aiken from Stanford University. It is provided as
a webservice, and is a closed source tool. Due to the closed nature of MOSS, it
is hard to effectively compare the AST comparison algorithm to it; only
comparing results and runtime is possible. The standard way of interfacing with
MOSS is through a Perl script, provided to a user upon registering for the
service. We feel that the \textit{Checksims} model and command line tools are
superior in the field of usability. In the authors experience, MOSS often had
problems with network timeouts and simply failing to report any results at all.
As \textit{Checksims} is run locally, it is not subject failed or slow network
connections.

Most importantly, the ability to run \textit{Checksims} locally removes any
ethical question of uploading named student assignments to remote machines that
are not under the control of the student's own academic institution.
