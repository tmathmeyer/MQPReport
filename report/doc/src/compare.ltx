\section{Comparison to Smith-Watermann}
\label{sec:discussComparison}
The need to create an AST comparison tool was realized when it was shown that
certain obfuscation techniques could be applied to blatantly plagerized code in
order to defeat the Smith-Waterman algorithm. An example scenario was created
in which a medium sized java file was created by copying a source file and
renaming every variable and class type. It was shown that this type of simple
obfuscation was enough to cause \textit{Checksims} to fail to recognize
plagerism that would be fairly obvious to a human. By creating an effective AST
comparison tool, it is hoped that theoretical gaps in the Smith-Watermann
algorithm are brought to light. The goal of comparing Smith-Waterman to an AST
comparison algorithm is to show the strengths and weaknesses of each when
applied to real world examples. This includes metrics like runtime and memory
consumption, in addition to false positive and missed instances of plagerism.
The results measured from both of these algorithms are discussed later in the
report.

\subsection{Time Complexity}
\label{sec:comparison-timecomplex}
\subsubsection{Big-O Notation}
The Checksims pipeline works by first reading in and concatinating every file
in each student's assignment. The next step, and first in the comparison
process, is to parse that text. This is a linear process; the amount of time
taken to parse every assignment corresponds linearly to the number of
assignments. The second step is to compare each current assignment to every
other current assignment, and each current assignment to each historical
assignment. This process corresponds quadradically to the number of assignments
present and the size of those assignments. In Mathematics and Computer Science,
these relationships are referred to as $O(n)$ and $O(n^{2})$ respectively. 

\subsubsection{Smith-Waterman}
The Smith-Waterman algorithm consists of two parts, each run separately by
\textit{Checksims}. The first step is to split the submission text by any of
three delimiters: newlines, space characters (such as space and tab), or at
every character. As discussed previously, this step runs in $O(n)$ time, so it
does not have a great effect on the overall runtime. The second step is to run
the lines through a comparison matrix and compute similar sequences. This is a
time consuming process, and because this step runs in $O(n^{2})$ time, it has a
significant impact on the overall runtime of the \textit{Checksims} program.

\subsubsection{AST Fingerprinting}
The simple implementation of AST fingerprinting also consists of two parts,
much like Smith-Waterman. The first step however is reletively time consuming,
as a tree must be constructed from the submitted source code. This result is
cached so that each assignment needs only to be parsed once. This overall has a
reletively small impact on the overall performance because it is part of the
$O(n)$ step of the process. Where AST Fingerprinting excels is in the
comparison, which is simply a sequence of hashmap lookups and some arithmetic.
As this step is run $O(n^{2})$ times, its runtime is significant to the overall
runtime.  Because the AST fingerprinting algorithm does not do expensive
calculations during the $O(n^{2})$ step, we can expect it to scale better than
the Smith-Waterman algorithm; that is, for large assignments, we expect AST
fingerprinting to be significantly faster than Smith-Waterman.  This is shown
to be true, as is discussed in the results section.

\subsection{Relationship to Manual inspection}
Based on the previous experience of TJM as a member of course staff, and his
having manually inspected assignments for many classes, two main things that
signal unauthorized copying can be identified. Structual similarities are one
such signal, and include such visual details such as indentation and spacing
between lines of code, as well as the organisation of control-flow within the
program. AST Fingerprinting is designed to detect similarities in control-flow
organisation, but may not pick up typographical similarities such as the
spelling of words and variable named, as the algorithm has no way of comparing
the tokens by content. Smith-Waterman does exactly the opposite of this,
detecting only similiarities in spelling and using tokenisation to completely
ignore structural differences. Prior experience as teaching staff have shown
that manually comparing structure yields a more accurate indication of
unauthorized copying when compared to typographical comparisons.

\section{Comparison to MOSS}
The Measure of Software Similarity, often referred to as MOSS, is the gold
standard for similarity detection. MOSS is developed by Professor Alex Aiken
from Stanford University\cite{moss-url}. It is provided as a webservice, and is
a closed source tool. Due to the closed nature of MOSS, it is hard to
effectively compare the AST comparison algorithm to it; only comparing results
and runtime is possible. The standard way of interfacing with MOSS is through a
Perl script, provided to a user upon registering for the service. We feel that
the \textit{Checksims} model and command line tools are superior in the field
of usability. MOSS often had problems with network timeouts and simply failing
to report any results at all.  As \textit{Checksims} is run locally, it is not
subject failed or slow network connections.  Perhaps most importantly, The
ability to run \textit{Checksims} locally removes any ethical question of
uploading named student assignments to a remote machine that is not under the
control of the student's own academic institution.
